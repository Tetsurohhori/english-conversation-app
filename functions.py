import streamlit as st
import os
import time
from pathlib import Path
from pydub import AudioSegment
from audio_recorder_streamlit import audio_recorder
from langchain.prompts import (
    ChatPromptTemplate,
    HumanMessagePromptTemplate,
    MessagesPlaceholder,
)
from langchain.schema import SystemMessage
from langchain.memory import ConversationSummaryBufferMemory
from langchain_openai import ChatOpenAI
from langchain.chains import ConversationChain
import constants as ct
from openai import OpenAIError, APIError, APIConnectionError, RateLimitError

def record_audio(audio_input_file_path):
    """
    éŸ³å£°å…¥åŠ›ã‚’å—ã‘å–ã£ã¦éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½œæˆ
    5ç§’é–“æ²ˆé»™ã™ã‚‹ã¨è‡ªå‹•çš„ã«éŒ²éŸ³ãŒå®Œäº†ã—ã¾ã™
    """

    audio_bytes = audio_recorder(
        key=f"audio_{audio_input_file_path}"
    )

    if audio_bytes:
        # éŸ³å£°ãƒ‡ãƒ¼ã‚¿ã‚’ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜
        with open(audio_input_file_path, 'wb') as audio_file:
            audio_file.write(audio_bytes)
    else:
        st.stop()

def transcribe_audio(audio_input_file_path):
    """
    éŸ³å£°å…¥åŠ›ãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰æ–‡å­—èµ·ã“ã—ãƒ†ã‚­ã‚¹ãƒˆã‚’å–å¾—
    Args:
        audio_input_file_path: éŸ³å£°å…¥åŠ›ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹
    """
    try:
        with open(audio_input_file_path, 'rb') as audio_input_file:
            transcript = st.session_state.openai_obj.audio.transcriptions.create(
                model="whisper-1",
                file=audio_input_file,
                language="en"
            )
        
        # éŸ³å£°å…¥åŠ›ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‰Šé™¤
        os.remove(audio_input_file_path)
        
        return transcript
        
    except RateLimitError:
        st.error("âš ï¸ APIåˆ©ç”¨åˆ¶é™ã«é”ã—ã¾ã—ãŸã€‚ã—ã°ã‚‰ãå¾…ã£ã¦ã‹ã‚‰å†åº¦ãŠè©¦ã—ãã ã•ã„ã€‚")
        st.stop()
    except APIConnectionError:
        st.error("âš ï¸ ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯æ¥ç¶šã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸã€‚ã‚¤ãƒ³ã‚¿ãƒ¼ãƒãƒƒãƒˆæ¥ç¶šã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚")
        st.stop()
    except APIError as e:
        st.error(f"âš ï¸ OpenAI APIã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}")
        st.stop()
    except Exception as e:
        st.error(f"âš ï¸ éŸ³å£°èªè­˜ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}")
        # ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¦ã‚‚ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‰Šé™¤
        if os.path.exists(audio_input_file_path):
            os.remove(audio_input_file_path)
        st.stop()

def save_to_wav(llm_response_audio, audio_output_file_path):
    """
    ä¸€æ—¦mp3å½¢å¼ã§éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ä½œæˆå¾Œã€wavå½¢å¼ã«å¤‰æ›
    Args:
        llm_response_audio: LLMã‹ã‚‰ã®å›ç­”ã®éŸ³å£°ãƒ‡ãƒ¼ã‚¿
        audio_output_file_path: å‡ºåŠ›å…ˆã®ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹
    """

    temp_audio_output_filename = f"{ct.AUDIO_OUTPUT_DIR}/temp_audio_output_{int(time.time())}.mp3"
    with open(temp_audio_output_filename, "wb") as temp_audio_output_file:
        temp_audio_output_file.write(llm_response_audio)
    
    audio_mp3 = AudioSegment.from_file(temp_audio_output_filename, format="mp3")
    audio_mp3.export(audio_output_file_path, format="wav")

    # éŸ³å£°å‡ºåŠ›ç”¨ã«ä¸€æ™‚çš„ã«ä½œã£ãŸmp3ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‰Šé™¤
    os.remove(temp_audio_output_filename)

def play_wav(audio_output_file_path, speed=1.0):
    """
    éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ã®èª­ã¿ä¸Šã’ï¼ˆStreamlitã®st.audioã‚’ä½¿ç”¨ï¼‰
    Args:
        audio_output_file_path: éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹
        speed: å†ç”Ÿé€Ÿåº¦ï¼ˆ1.0ãŒé€šå¸¸é€Ÿåº¦ã€0.5ã§åŠåˆ†ã®é€Ÿã•ã€2.0ã§å€é€Ÿãªã©ï¼‰
    """
    try:
        # éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ã®èª­ã¿è¾¼ã¿
        audio = AudioSegment.from_wav(audio_output_file_path)
        
        # é€Ÿåº¦ã‚’å¤‰æ›´
        if speed != 1.0:
            # frame_rateã‚’å¤‰æ›´ã™ã‚‹ã“ã¨ã§é€Ÿåº¦ã‚’èª¿æ•´
            modified_audio = audio._spawn(
                audio.raw_data, 
                overrides={"frame_rate": int(audio.frame_rate * speed)}
            )
            # å…ƒã®frame_rateã«æˆ»ã™ã“ã¨ã§æ­£å¸¸å†ç”Ÿã•ã›ã‚‹ï¼ˆãƒ”ãƒƒãƒã‚’ä¿æŒã—ãŸã¾ã¾é€Ÿåº¦ã ã‘å¤‰æ›´ï¼‰
            modified_audio = modified_audio.set_frame_rate(audio.frame_rate)
            modified_audio.export(audio_output_file_path, format="wav")
        
        # éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã¿
        with open(audio_output_file_path, 'rb') as audio_file:
            audio_bytes = audio_file.read()
        
        # Streamlitã®audioæ©Ÿèƒ½ã§å†ç”Ÿ
        st.audio(audio_bytes, format='audio/wav')
        
        # HTML5ã‚ªãƒ¼ãƒ‡ã‚£ã‚ªã‚¿ã‚°ã‚’ä½¿ç”¨ã—ã¦è‡ªå‹•å†ç”Ÿã‚’è©¦ã¿ã‚‹
        import base64
        audio_base64 = base64.b64encode(audio_bytes).decode()
        audio_html = f"""
        <audio autoplay>
            <source src="data:audio/wav;base64,{audio_base64}" type="audio/wav">
        </audio>
        """
        st.markdown(audio_html, unsafe_allow_html=True)
        
    except Exception as e:
        st.error(f"âš ï¸ éŸ³å£°å†ç”Ÿä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}")
    finally:
        # éŸ³å£°å†ç”Ÿå¾Œã«å°‘ã—å¾…ã£ã¦ã‹ã‚‰ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‰Šé™¤
        time.sleep(0.5)
        if os.path.exists(audio_output_file_path):
            os.remove(audio_output_file_path)

def create_chain(system_template):
    """
    LLMã«ã‚ˆã‚‹å›ç­”ç”Ÿæˆç”¨ã®Chainä½œæˆ
    """

    prompt = ChatPromptTemplate.from_messages([
        SystemMessage(content=system_template),
        MessagesPlaceholder(variable_name="history"),
        HumanMessagePromptTemplate.from_template("{input}")
    ])
    chain = ConversationChain(
        llm=st.session_state.llm,
        memory=st.session_state.memory,
        prompt=prompt
    )

    return chain

def create_problem_prompt_with_context(base_prompt, topic, situation):
    """
    ãƒˆãƒ”ãƒƒã‚¯ã¨ã‚·ãƒãƒ¥ã‚¨ãƒ¼ã‚·ãƒ§ãƒ³ã‚’è€ƒæ…®ã—ãŸå•é¡Œæ–‡ç”Ÿæˆãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ä½œæˆ
    Args:
        base_prompt: åŸºæœ¬ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆ
        topic: é¸æŠã•ã‚ŒãŸãƒˆãƒ”ãƒƒã‚¯
        situation: é¸æŠã•ã‚ŒãŸã‚·ãƒãƒ¥ã‚¨ãƒ¼ã‚·ãƒ§ãƒ³
    """
    additional_context = ""
    
    if topic and topic != "ãƒ©ãƒ³ãƒ€ãƒ ":
        topic_desc = ct.TOPIC_OPTIONS.get(topic, "")
        if topic_desc:
            additional_context += f"\n    Focus on: {topic_desc}"
    
    if situation and situation != "æŒ‡å®šãªã—":
        situation_desc = ct.SITUATION_OPTIONS.get(situation, "")
        if situation_desc:
            additional_context += f"\n    Situation: {situation_desc}"
    
    # åŸºæœ¬ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã«è¿½åŠ ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚’çµ„ã¿è¾¼ã‚€
    if additional_context:
        modified_prompt = base_prompt.rstrip() + additional_context + "\n"
        return modified_prompt
    
    return base_prompt

def create_problem_and_play_audio():
    """
    å•é¡Œç”Ÿæˆã¨éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ã®å†ç”Ÿ
    Args:
        chain: å•é¡Œæ–‡ç”Ÿæˆç”¨ã®Chain
        speed: å†ç”Ÿé€Ÿåº¦ï¼ˆ1.0ãŒé€šå¸¸é€Ÿåº¦ã€0.5ã§åŠåˆ†ã®é€Ÿã•ã€2.0ã§å€é€Ÿãªã©ï¼‰
        openai_obj: OpenAIã®ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆ
    """
    try:
        # å•é¡Œæ–‡ã‚’ç”Ÿæˆã™ã‚‹Chainã‚’å®Ÿè¡Œã—ã€å•é¡Œæ–‡ã‚’å–å¾—
        problem = st.session_state.chain_create_problem.predict(input="")

        # LLMã‹ã‚‰ã®å›ç­”ã‚’éŸ³å£°ãƒ‡ãƒ¼ã‚¿ã«å¤‰æ›
        llm_response_audio = st.session_state.openai_obj.audio.speech.create(
            model="tts-1",
            voice="alloy",
            input=problem
        )

        # éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ã®ä½œæˆ
        audio_output_file_path = f"{ct.AUDIO_OUTPUT_DIR}/audio_output_{int(time.time())}.wav"
        save_to_wav(llm_response_audio.content, audio_output_file_path)

        # éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ã®èª­ã¿ä¸Šã’
        play_wav(audio_output_file_path, st.session_state.speed)

        return problem, llm_response_audio
        
    except RateLimitError:
        st.error("âš ï¸ APIåˆ©ç”¨åˆ¶é™ã«é”ã—ã¾ã—ãŸã€‚ã—ã°ã‚‰ãå¾…ã£ã¦ã‹ã‚‰å†åº¦ãŠè©¦ã—ãã ã•ã„ã€‚")
        st.stop()
    except APIConnectionError:
        st.error("âš ï¸ ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯æ¥ç¶šã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸã€‚ã‚¤ãƒ³ã‚¿ãƒ¼ãƒãƒƒãƒˆæ¥ç¶šã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚")
        st.stop()
    except APIError as e:
        st.error(f"âš ï¸ OpenAI APIã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}")
        st.stop()
    except Exception as e:
        st.error(f"âš ï¸ å•é¡Œæ–‡ç”Ÿæˆä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}")
        st.stop()

def create_evaluation():
    """
    ãƒ¦ãƒ¼ã‚¶ãƒ¼å…¥åŠ›å€¤ã®è©•ä¾¡ç”Ÿæˆ
    """
    try:
        llm_response_evaluation = st.session_state.chain_evaluation.predict(input="")
        return llm_response_evaluation
        
    except RateLimitError:
        st.error("âš ï¸ APIåˆ©ç”¨åˆ¶é™ã«é”ã—ã¾ã—ãŸã€‚ã—ã°ã‚‰ãå¾…ã£ã¦ã‹ã‚‰å†åº¦ãŠè©¦ã—ãã ã•ã„ã€‚")
        st.stop()
    except APIConnectionError:
        st.error("âš ï¸ ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯æ¥ç¶šã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸã€‚ã‚¤ãƒ³ã‚¿ãƒ¼ãƒãƒƒãƒˆæ¥ç¶šã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚")
        st.stop()
    except APIError as e:
        st.error(f"âš ï¸ OpenAI APIã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}")
        st.stop()
    except Exception as e:
        st.error(f"âš ï¸ è©•ä¾¡ç”Ÿæˆä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}")
        st.stop()

def extract_score(evaluation_text):
    """
    è©•ä¾¡ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰ã‚¹ã‚³ã‚¢ã‚’æŠ½å‡º
    Args:
        evaluation_text: è©•ä¾¡çµæœã®ãƒ†ã‚­ã‚¹ãƒˆ
    Returns:
        score: ã‚¹ã‚³ã‚¢ï¼ˆ0-100ï¼‰ã€æŠ½å‡ºã§ããªã„å ´åˆã¯None
    """
    import re
    
    # ã‚¹ã‚³ã‚¢ã®ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’æ¤œç´¢ï¼ˆä¾‹: "ã‚¹ã‚³ã‚¢: 85/100ç‚¹"ï¼‰
    score_pattern = r'ã‚¹ã‚³ã‚¢[:ï¼š]\s*(\d+)/100ç‚¹'
    match = re.search(score_pattern, evaluation_text)
    
    if match:
        return int(match.group(1))
    return None

def display_score_badge(score):
    """
    ã‚¹ã‚³ã‚¢ã«å¿œã˜ãŸãƒãƒƒã‚¸ã‚’è¡¨ç¤º
    Args:
        score: ã‚¹ã‚³ã‚¢ï¼ˆ0-100ï¼‰
    """
    if score is None:
        return
    
    if score >= 90:
        st.success(f"ğŸ‰ ç´ æ™´ã‚‰ã—ã„ï¼ ã‚¹ã‚³ã‚¢: {score}/100ç‚¹")
    elif score >= 70:
        st.info(f"ğŸ‘ è‰¯ãã§ãã¾ã—ãŸï¼ ã‚¹ã‚³ã‚¢: {score}/100ç‚¹")
    elif score >= 50:
        st.warning(f"ğŸ“ ã‚‚ã†å°‘ã—ï¼ ã‚¹ã‚³ã‚¢: {score}/100ç‚¹")
    else:
        st.error(f"ğŸ’ª é ‘å¼µã‚Šã¾ã—ã‚‡ã†ï¼ ã‚¹ã‚³ã‚¢: {score}/100ç‚¹")